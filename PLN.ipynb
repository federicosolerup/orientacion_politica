{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Trabajo Final de la materia Procesamiento de Lenguaje Natural\n",
    "- python -m pip install --upgrade pip\n",
    "- pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "- pip install transformers datasets scikit-learn pandas ipywidgets spacy\n",
    "- python -m spacy download es_core_news_sm"
   ],
   "id": "549d6bb6ef7fb5ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T21:27:12.705849Z",
     "start_time": "2025-06-14T21:26:58.271446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuración del logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "94846d7344708463",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PARTE 1: Preprocesamiento del corpus",
   "id": "316a5b58046f7c61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T20:38:19.672849Z",
     "start_time": "2025-06-13T20:38:19.657225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PreprocesadorTexto:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    def segmentar_frases(self, texto):\n",
    "        return [f.strip() for f in re.split(r'(?<=[.!?])\\s+', texto) if f.strip()]\n",
    "\n",
    "    def extraer_entidades(self, texto):\n",
    "        try:\n",
    "            doc = self.nlp(texto)\n",
    "            return list(set(ent.text for ent in doc.ents if ent.label_ in {\"PER\", \"ORG\", \"LOC\"}))\n",
    "        except:\n",
    "            return []"
   ],
   "id": "49725bc92f1f3bd5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T20:38:19.704098Z",
     "start_time": "2025-06-13T20:38:19.688474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ProcesadorCorpusSinModelo:\n",
    "    def __init__(self, corpus_dir, output_dir, preprocesador):\n",
    "        self.corpus_dir = corpus_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.preprocesador = preprocesador\n",
    "\n",
    "    def procesar(self):\n",
    "        inicio = time.time()\n",
    "        archivos = []  # (fn, txt, categoria)\n",
    "\n",
    "        for categoria in ['izquierda', 'derecha', 'neutral']:\n",
    "            path = os.path.join(self.corpus_dir, categoria)\n",
    "            if not os.path.isdir(path):\n",
    "                continue\n",
    "            for fn in os.listdir(path):\n",
    "                if fn.endswith('.txt'):\n",
    "                    with open(os.path.join(path, fn), encoding='utf-8') as f:\n",
    "                        txt = f.read()\n",
    "                    archivos.append((fn, txt, categoria))\n",
    "\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        for fn, txt, categoria in tqdm(archivos, desc='Preprocesando'):\n",
    "            frases = self.preprocesador.segmentar_frases(txt)\n",
    "            frases_procesadas = []\n",
    "\n",
    "            for frase in frases:\n",
    "                entidades = self.preprocesador.extraer_entidades(frase)\n",
    "                frases_procesadas.append({\n",
    "                    \"frase\": frase,\n",
    "                    \"entidades_detectadas\": entidades\n",
    "                })\n",
    "\n",
    "            out_json = {\n",
    "                \"etiqueta_original\": categoria,\n",
    "                \"frases\": frases_procesadas,\n",
    "                \"entidades_mencionadas\": self.preprocesador.extraer_entidades(txt)\n",
    "            }\n",
    "\n",
    "            out_path = os.path.join(self.output_dir, fn.replace(\".txt\", \".json\"))\n",
    "            with open(out_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(out_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        duracion = int(time.time() - inicio)\n",
    "        print(f\"Listo. Tiempo total: {duracion // 60}m {duracion % 60}s\")"
   ],
   "id": "98a13311028146fb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T21:50:25.531860Z",
     "start_time": "2025-06-13T20:38:19.719723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_dir = os.getcwd()\n",
    "corpus_dir = os.path.join(base_dir, 'transcripciones')\n",
    "output_dir = os.path.join(base_dir, 'resultados')\n",
    "\n",
    "preprocesador = PreprocesadorTexto()\n",
    "procesador = ProcesadorCorpusSinModelo(corpus_dir, output_dir, preprocesador)\n",
    "procesador.procesar()"
   ],
   "id": "6fd32946f7de6284",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocesando: 100%|██████████| 3115/3115 [1:12:04<00:00,  1.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listo. Tiempo total: 72m 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PARTE 2: Análisis de entidades mencionadas",
   "id": "e4794a75c6627337"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T21:50:27.017765Z",
     "start_time": "2025-06-13T21:50:25.578734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ruta a la carpeta de resultados\n",
    "carpeta_resultados = \"./resultados\"\n",
    "\n",
    "# Contador de todas las entidades mencionadas\n",
    "contador_entidades = Counter()\n",
    "\n",
    "# Recorrer todos los archivos JSON\n",
    "for ruta in glob.glob(os.path.join(carpeta_resultados, \"*.json\")):\n",
    "    try:\n",
    "        with open(ruta, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            entidades = data.get(\"entidades_mencionadas\", [])\n",
    "            contador_entidades.update(entidades)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en {ruta}: {e}\")\n",
    "\n",
    "# Crear DataFrame ordenado por frecuencia\n",
    "df_entidades = pd.DataFrame(contador_entidades.items(), columns=[\"entidad\", \"frecuencia\"])\n",
    "df_entidades = df_entidades.sort_values(by=\"frecuencia\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Guardar CSV en la carpeta principal del proyecto\n",
    "df_entidades.to_csv(\"entidades_frecuentes.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Mostrar vista previa\n",
    "df_entidades.head(50)"
   ],
   "id": "38eba4a6ab19ee98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           entidad  frecuencia\n",
       "0                                ¿        2140\n",
       "1                        Argentina        1686\n",
       "2                            Bueno        1529\n",
       "3                     la Argentina        1299\n",
       "4                         Cristina        1251\n",
       "5                Alberto Fernández        1232\n",
       "6                           Estado        1046\n",
       "7                            Macri        1025\n",
       "8                             ¿Qué        1018\n",
       "9   Cristina Fernández de Kirchner         968\n",
       "10                           Milei         945\n",
       "11                           Vamos         915\n",
       "12                             Así         765\n",
       "13                          Además         742\n",
       "14                         Después         736\n",
       "15                         También         736\n",
       "16                        Congreso         725\n",
       "17                    Javier Milei         693\n",
       "18                            Ayer         690\n",
       "19                  Mauricio Macri         689\n",
       "20                         Alberto         663\n",
       "21                    Sergio Massa         563\n",
       "22               Cristina Kirchner         553\n",
       "23               Patricia Bullrich         530\n",
       "24                  Estados Unidos         520\n",
       "25       provincia de Buenos Aires         517\n",
       "26                             Acá         502\n",
       "27                             Qué         493\n",
       "28                           Massa         477\n",
       "29                            Está         472\n",
       "30                           llegó         464\n",
       "31                 Néstor Kirchner         433\n",
       "32                          Senado         428\n",
       "33                 Máximo Kirchner         426\n",
       "34                           Claro         419\n",
       "35                          Quiero         413\n",
       "36                   Axel Kicillof         389\n",
       "37                          Brasil         386\n",
       "38                    Buenos Aires         384\n",
       "39                         Primero         379\n",
       "40                   Banco Central         377\n",
       "41                      La Cámpora         370\n",
       "42                          ¿Quién         368\n",
       "43                       Venezuela         362\n",
       "44                        Kirchner         359\n",
       "45                            Mirá         358\n",
       "46                         Ustedes         355\n",
       "47                            Mira         347\n",
       "48                           ¿Cuál         342\n",
       "49                        Kicillof         342"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entidad</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿</td>\n",
       "      <td>2140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bueno</td>\n",
       "      <td>1529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>la Argentina</td>\n",
       "      <td>1299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cristina</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alberto Fernández</td>\n",
       "      <td>1232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Estado</td>\n",
       "      <td>1046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Macri</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>¿Qué</td>\n",
       "      <td>1018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cristina Fernández de Kirchner</td>\n",
       "      <td>968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Milei</td>\n",
       "      <td>945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Vamos</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Así</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Además</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Después</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>También</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Congreso</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Javier Milei</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ayer</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mauricio Macri</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Alberto</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sergio Massa</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Cristina Kirchner</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Patricia Bullrich</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>provincia de Buenos Aires</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Acá</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Qué</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Massa</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Está</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>llegó</td>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Néstor Kirchner</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Senado</td>\n",
       "      <td>428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Máximo Kirchner</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Claro</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Quiero</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Axel Kicillof</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Brasil</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Primero</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Banco Central</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>La Cámpora</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>¿Quién</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Venezuela</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Kirchner</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Mirá</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Ustedes</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Mira</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>¿Cuál</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Kicillof</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T21:38:21.388931Z",
     "start_time": "2025-06-14T21:38:21.276999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Lista manual de palabras irrelevantes detectadas manualmente\n",
    "irrelevantes = {\n",
    "    \"ayer\", \"acá\", \"mira\", \"mirá\", \"miren\", \"obviamente\", \"corte\", \"digo\", \"mire\",\n",
    "    \"gracias\", \"vos\", \"tenés\", \"ojalá\", \"eh\", \"mañana\", \"apellidos\", \"nombres\",\n",
    "    \"dios\", \"perdón\", \"escuchémoslo\", \"jefa\", \"che\", \"¿\", \"bueno\", \"qué\", \"vamos\",\n",
    "    \"así\", \"además\", \"después\", \"también\", \"está\", \"llegó\", \"claro\", \"quiero\",\n",
    "    \"primero\", \"¿quién\", \"¿cuál\", \"tenía\", \"cambio\", \"había\", \"sí\", \"ahí\", \"juntos\",\n",
    "    \"más\", \"según\", \"hola\", \"estoy\", \"ningún\", \"dale\", \"cómo\", \"esa\", \"papa\", \"jamás\",\n",
    "    \"coherencia\", \"empezó\", \"ah\", \"tenemos\", \"van\", \"bien\", \"me\", \"nos\", \"todavía\",\n",
    "    \"generó\", \"dicen\", \"recién\", \"cuál\", \"espero\", \"habló\", \"voy\", \"insisto\", \"allí\",\n",
    "    \"él\", \"unión\", \"cohesión\", \"ustedes\", \"jorge\", \"le\", \"k\", \"recuerdo\", \"decía\", \"boca\",\n",
    "    \"chicos\", \"ojo\", \"usted\", \"ortografía\", \"segundo\", \"pablo\", \"luego\", \"esos\", \"lamentablemente\",\n",
    "    \"podría\", \"poneme\", \"mauro\", \"quilombo\", \"somos\", \"miralo\", \"nuestro\", \"habra\", \"quieren\",\n",
    "    \"andres\", \"míralo\", \"habrá\", \"nada\", \"corrección\", \"whatsapp\", \"repito\", \"encontré\",\n",
    "    \"aquí\", \"afi\", \"anoche\", \"señor\"\n",
    "}\n",
    "\n",
    "# Cargar CSV original\n",
    "df = pd.read_csv(\"entidades_frecuentes.csv\")\n",
    "df = df[df[\"entidad\"].notna()]\n",
    "df[\"entidad\"] = df[\"entidad\"].str.strip().str.lower()\n",
    "\n",
    "# Filtrar: eliminar entradas no alfabéticas y palabras irrelevantes\n",
    "df = df[df[\"entidad\"].str.isalpha()]\n",
    "df = df[~df[\"entidad\"].isin(irrelevantes)]\n",
    "\n",
    "# Agrupar por entidad en caso de duplicados\n",
    "df = df.groupby(\"entidad\", as_index=False).sum()\n",
    "\n",
    "# Seleccionar las 100 más frecuentes\n",
    "df_top100 = df.sort_values(by=\"frecuencia\", ascending=False).head(100)\n",
    "\n",
    "# Guardar resultado\n",
    "df_top100.to_csv(\"entidades_top100_limpias.csv\", index=False)\n",
    "\n",
    "print(\"Archivo 'entidades_top100_limpias.csv' creado con 100 entidades útiles.\")\n",
    "df_top100.head(100)\n"
   ],
   "id": "67fbdb56acf7dd06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'entidades_top100_limpias.csv' creado con 100 entidades útiles.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         entidad  frecuencia\n",
       "1064   argentina        1736\n",
       "3971    cristina        1253\n",
       "5473      estado        1046\n",
       "8850       macri        1027\n",
       "9591       milei         954\n",
       "...          ...         ...\n",
       "10286  nicaragua          81\n",
       "3168      chubut          80\n",
       "2734      carlos          78\n",
       "11302       perú          77\n",
       "6199    gabinete          77\n",
       "\n",
       "[100 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entidad</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>argentina</td>\n",
       "      <td>1736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>cristina</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5473</th>\n",
       "      <td>estado</td>\n",
       "      <td>1046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8850</th>\n",
       "      <td>macri</td>\n",
       "      <td>1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9591</th>\n",
       "      <td>milei</td>\n",
       "      <td>954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10286</th>\n",
       "      <td>nicaragua</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3168</th>\n",
       "      <td>chubut</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2734</th>\n",
       "      <td>carlos</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11302</th>\n",
       "      <td>perú</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6199</th>\n",
       "      <td>gabinete</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T21:39:20.221678Z",
     "start_time": "2025-06-14T21:39:01.140506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cargar entidades útiles\n",
    "df = pd.read_csv(\"entidades_top100_limpias.csv\")\n",
    "entidades_utiles = set(df[\"entidad\"].str.lower())\n",
    "\n",
    "# Definir rutas\n",
    "carpeta_jsons = \"resultados\"\n",
    "carpeta_transcripciones = \"transcripciones\"\n",
    "carpeta_salida_base = \"resultados_filtrados\"\n",
    "subcarpetas = [\"izquierda\", \"derecha\", \"neutral\"]\n",
    "\n",
    "# Crear subcarpetas\n",
    "for sub in subcarpetas:\n",
    "    os.makedirs(os.path.join(carpeta_salida_base, sub), exist_ok=True)\n",
    "\n",
    "def entidades_presentes(frase):\n",
    "    palabras = frase.lower().split()\n",
    "    return [ent for ent in entidades_utiles if ent in palabras]\n",
    "\n",
    "# Procesar JSONs\n",
    "for archivo in os.listdir(carpeta_jsons):\n",
    "    if not archivo.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    texto_id = archivo.replace(\".json\", \"\")\n",
    "    etiqueta = None\n",
    "    for sub in subcarpetas:\n",
    "        posible_ruta = os.path.join(carpeta_transcripciones, sub, texto_id + \".txt\")\n",
    "        if os.path.exists(posible_ruta):\n",
    "            etiqueta = sub\n",
    "            break\n",
    "\n",
    "    if etiqueta is None:\n",
    "        print(f\"No se encontró transcripción para: {archivo}\")\n",
    "        continue\n",
    "\n",
    "    with open(os.path.join(carpeta_jsons, archivo), \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    frases_relevantes = []\n",
    "    for f in data.get(\"frases\", []):\n",
    "        entidades = entidades_presentes(f[\"frase\"])\n",
    "        if entidades:\n",
    "            f_mod = f.copy()\n",
    "            f_mod[\"entidades_detectadas\"] = entidades\n",
    "            frases_relevantes.append(f_mod)\n",
    "\n",
    "    if frases_relevantes:\n",
    "        nuevo_json = {\n",
    "            \"etiqueta_original\": etiqueta,\n",
    "            \"frases\": frases_relevantes\n",
    "        }\n",
    "        ruta_salida = os.path.join(carpeta_salida_base, etiqueta, archivo)\n",
    "        with open(ruta_salida, \"w\", encoding=\"utf-8\") as out_f:\n",
    "            json.dump(nuevo_json, out_f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Listo. JSONs filtrados guardados en 'resultados_filtrados/'.\")"
   ],
   "id": "19442f64d4e22805",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listo. JSONs filtrados guardados en 'resultados_filtrados/'.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T21:40:25.451515Z",
     "start_time": "2025-06-14T21:40:22.677223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cargar entidades útiles\n",
    "df_entidades = pd.read_csv(\"entidades_top100_limpias.csv\")\n",
    "entidades_significativas = set(df_entidades[\"entidad\"])  # ya están en minúsculas\n",
    "\n",
    "# Rutas\n",
    "carpeta_jsons = \"resultados_filtrados\"\n",
    "salida_csv = \"train_significativo.csv\"\n",
    "\n",
    "# Preparar filas\n",
    "filas = []\n",
    "\n",
    "for subdir in ['izquierda', 'derecha']:\n",
    "    label = 0 if subdir == 'izquierda' else 1\n",
    "    ruta_subcarpeta = os.path.join(carpeta_jsons, subdir)\n",
    "\n",
    "    for archivo in os.listdir(ruta_subcarpeta):\n",
    "        if not archivo.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        ruta = os.path.join(ruta_subcarpeta, archivo)\n",
    "        with open(ruta, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for frase in data.get(\"frases\", []):\n",
    "            entidades = frase.get(\"entidades_detectadas\", [])\n",
    "            if any(ent in entidades_significativas for ent in entidades):\n",
    "                filas.append({\n",
    "                    \"archivo\": archivo,\n",
    "                    \"texto\": frase[\"frase\"],\n",
    "                    \"label\": label\n",
    "                })\n",
    "\n",
    "# Guardar CSV\n",
    "df_final = pd.DataFrame(filas)\n",
    "df_final.to_csv(salida_csv, index=False, encoding=\"utf-8\")\n",
    "print(f\"Dataset generado con {len(df_final)} frases en {salida_csv}\")"
   ],
   "id": "9606863fd9f0a2e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generado con 29083 frases en train_significativo.csv\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PARTE 3: Fine-tuning de BERT",
   "id": "22477c43edbe1bfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T03:07:44.368828Z",
     "start_time": "2025-06-14T00:53:06.710317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verificar y mostrar dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Cargar tokenizer y modelo base\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\", num_labels=2)\n",
    "\n",
    "# Función para chunkear texto\n",
    "def chunkear(texto, tokenizer, max_length=510):\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    return [tokenizer.convert_tokens_to_string(tokens[i:i + max_length])\n",
    "            for i in range(0, len(tokens), max_length)] or [texto]\n",
    "\n",
    "# Leer CSV y generar chunks\n",
    "df = pd.read_csv(\"train_significativo.csv\")\n",
    "data_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    chunks = chunkear(row[\"texto\"], tokenizer)\n",
    "    for chunk in chunks:\n",
    "        if chunk.strip():\n",
    "            data_rows.append({\"text\": chunk, \"label\": row[\"label\"]})\n",
    "\n",
    "# Crear dataset y dividir\n",
    "dataset = Dataset.from_dict({\n",
    "    \"text\": [r[\"text\"] for r in data_rows],\n",
    "    \"label\": [r[\"label\"] for r in data_rows]\n",
    "})\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Configurar entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_significativo\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Entrenar (reanudar si hay checkpoint)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split[\"train\"],\n",
    "    eval_dataset=split[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "# Guardar modelo fine-tuneado\n",
    "model.save_pretrained(\"modelo_finetuneado_significativo\")\n",
    "tokenizer.save_pretrained(\"modelo_finetuneado_significativo\")"
   ],
   "id": "497833efe4e16fd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/40800 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f4425b55a59437b93cad04937fad0e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fedez\\AppData\\Local\\Temp\\ipykernel_10652\\3883577624.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12240' max='12240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12240/12240 2:13:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.540615</td>\n",
       "      <td>0.812377</td>\n",
       "      <td>0.811533</td>\n",
       "      <td>0.812377</td>\n",
       "      <td>0.811614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.942421</td>\n",
       "      <td>0.807353</td>\n",
       "      <td>0.809029</td>\n",
       "      <td>0.807353</td>\n",
       "      <td>0.807916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('modelo_finetuneado_significativo\\\\tokenizer_config.json',\n",
       " 'modelo_finetuneado_significativo\\\\special_tokens_map.json',\n",
       " 'modelo_finetuneado_significativo\\\\vocab.txt',\n",
       " 'modelo_finetuneado_significativo\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PARTE 4: Análisis de texto con el modelo fine-tuneado",
   "id": "237edaf7cdbb1926"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T03:10:43.131578Z",
     "start_time": "2025-06-14T03:10:43.100329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AnalizadorIdeologico:\n",
    "    def __init__(self, config):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() and config.get('device') != 'cpu' else 'cpu'\n",
    "        model_path = config['model_path']\n",
    "        if not os.path.isdir(model_path) or not os.path.exists(os.path.join(model_path, 'config.json')):\n",
    "            raise FileNotFoundError(f\"No se encontró el modelo BERT fine-tuneado en {model_path}\")\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        self.model_cls = BertForSequenceClassification.from_pretrained(model_path).to(self.device)\n",
    "        self.model_cls.eval()\n",
    "\n",
    "    def _chunkear(self, texto, max_length=510):\n",
    "        tokens = self.tokenizer.tokenize(texto)\n",
    "        return [self.tokenizer.convert_tokens_to_string(tokens[i:i + max_length])\n",
    "                for i in range(0, len(tokens), max_length)] or [texto]\n",
    "\n",
    "    def _predecir_chunk(self, chunk_text):\n",
    "        try:\n",
    "            inputs = self.tokenizer(chunk_text, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model_cls(**inputs)\n",
    "                return torch.softmax(outputs.logits, dim=1)[0][1].item()\n",
    "        except:\n",
    "            return 0.5\n",
    "\n",
    "    def analizar_frases(self, frases):\n",
    "        frases_detalle = []\n",
    "\n",
    "        for frase in frases:\n",
    "            chunks = self._chunkear(frase[\"frase\"])\n",
    "            chunk_scores = [self._predecir_chunk(chunk) for chunk in chunks if chunk.strip()]\n",
    "            if chunk_scores:\n",
    "                score_frase = sum(chunk_scores) / len(chunk_scores)\n",
    "                etiqueta = \"derecha\" if score_frase > 0.5 else \"izquierda\"\n",
    "                frases_detalle.append({\n",
    "                    \"frase\": frase[\"frase\"],\n",
    "                    \"score\": round(score_frase, 3),\n",
    "                    \"etiqueta\": etiqueta\n",
    "                })\n",
    "\n",
    "        if not frases_detalle:\n",
    "            return None\n",
    "\n",
    "        scores = [f[\"score\"] for f in frases_detalle]\n",
    "        conteo_izq = sum(1 for s in scores if s <= 0.5)\n",
    "        conteo_der = len(scores) - conteo_izq\n",
    "        score_promedio = sum(scores) / len(scores)\n",
    "        etiqueta_final = \"derecha\" if score_promedio > 0.5 else \"izquierda\"\n",
    "\n",
    "        return {\n",
    "            \"perfil_global_del_texto\": {\n",
    "                \"conteo\": {\"izquierda\": conteo_izq, \"derecha\": conteo_der},\n",
    "                \"score_promedio\": round(score_promedio, 3),\n",
    "                \"etiqueta_final\": etiqueta_final\n",
    "            },\n",
    "            \"frases\": frases_detalle,\n",
    "            \"detalles\": {\"frases_analizadas\": len(frases_detalle)}\n",
    "        }"
   ],
   "id": "6776a5ea2400f651",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T03:38:01.730294Z",
     "start_time": "2025-06-14T03:10:48.577843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cargar clases necesarias\n",
    "analizador = AnalizadorIdeologico({\"model_path\": \"modelo_finetuneado_significativo\"})\n",
    "\n",
    "# Directorios\n",
    "input_dir = \"resultados_filtrados/neutral\"\n",
    "output_base = \"resultados_significativos\"\n",
    "Path(os.path.join(output_base, \"izquierda\")).mkdir(parents=True, exist_ok=True)\n",
    "Path(os.path.join(output_base, \"derecha\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Procesar cada archivo\n",
    "for file in os.listdir(input_dir):\n",
    "    if not file.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    with open(os.path.join(input_dir, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    frases = data.get(\"frases\", [])\n",
    "    if not frases:\n",
    "        continue\n",
    "\n",
    "    resultado = analizador.analizar_frases(frases)\n",
    "    if resultado is None:\n",
    "        continue\n",
    "\n",
    "    resultado[\"etiqueta_original\"] = data.get(\"etiqueta_original\", \"\")\n",
    "    output_path = os.path.join(output_base, resultado[\"perfil_global_del_texto\"][\"etiqueta_final\"], file)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(resultado, f_out, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Clasificación completada. JSONs guardados en resultados_significativos/izquierda o /derecha.\")"
   ],
   "id": "f3934ee9b017b0a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación completada. JSONs guardados en resultados_significativos/izquierda o /derecha.\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
